# Java 21 Stream API Advanced Cheatsheet

## What Are Streams? (Overview)

Java Streams provide a high-level API for processing sequences of data in a **pipeline** of functions, enabling a functional programming style for collections and other data sources. A stream pipeline consists of a **source** (e.g. a collection, array, generator, I/O channel), zero or more **intermediate operations**, and a **terminal operation**. Streams do *not* store data themselves – they are not a data structure and cannot mutate the original data; they only transform data or produce results. Streams are evaluated **lazily**: intermediate operations (such as filtering or mapping) are not executed until a terminal operation is invoked. This means you can chain multiple transformations, and they won’t actually run until the final result is needed. Once a terminal operation executes, the stream pipeline is consumed and cannot be reused. To reuse data, you must create a new stream or collect the results into a collection and stream that if needed.

* **Sequential vs. Parallel**: Streams can run in serial (default) or in parallel. A *parallel stream* divides the workload across multiple threads (using the common ForkJoinPool) to potentially improve performance on large data sets. However, parallelism introduces overhead and complexity, so use it only when appropriate (e.g. CPU-intensive processing on large collections) and when thread-safety of operations can be guaranteed. We cover more on parallel streams in a later section.

## Creating Streams

You can create streams from various data sources common in backend applications:

* **From Collections**: Every Collection in Java has a `stream()` method. For example:

  ```java
  List<Order> orders = getOrdersFromDB();
  Stream<Order> orderStream = orders.stream();         // sequential stream
  Stream<Order> parallelOrderStream = orders.parallelStream(); // parallel stream
  ```

  Use `parallelStream()` *sparingly* for large workloads – by default it uses a common thread pool.

* **From Arrays**: Use `Arrays.stream(array)` or `Stream.of(arrayElements...)`:

  ```java
  String[] names = {"Alice", "Bob", "Charlie"};
  Stream<String> nameStream = Arrays.stream(names);
  Stream<String> nameStream2 = Stream.of("Alice", "Bob", "Charlie");
  ```

* **From Generators/Builders**:

  * `Stream.generate(Supplier)` creates an infinite (or until short-circuited) stream of values generated by a supplier:

    ```java
    Stream<Double> randoms = Stream.generate(Math::random).limit(5);
    randoms.forEach(System.out::println);  // 5 random numbers
    ```
  * `Stream.iterate(seed, unaryOperator)` creates an infinite iterative stream. An overloaded form `Stream.iterate(seed, predicate, unaryOperator)` (since Java 9) produces a finite stream by stopping when the predicate is false:

    ```java
    Stream<Integer> squares = Stream.iterate(1, n -> n <= 5, n -> n * 2);
    squares.forEach(System.out::println);  // 1, 2, 4 (stops when next 8 > 5)
    ```

* **From Files or I/O**: For example, `Files.lines(Path)` returns a `Stream<String>` for lines in a text file (remember to close or use try-with-resources since it's I/O). Similarly, many other APIs provide stream methods, e.g. `BufferedReader.lines()` or `Pattern.splitAsStream()`.

* **From Optionals**: Java 9 added `Optional.stream()` which returns a stream of zero or one element contained in the Optional. This is useful to integrate with other stream logic without explicit checks:

  ```java
  Optional<User> optUser = findUser(id);
  optUser.stream().flatMap(user -> user.getRoles().stream())...
  ```

  Here, if the user is present, we stream the roles; if not, it contributes nothing (empty stream).

* **Single or Empty Streams**: Use `Stream.of(value)` for a single-element stream, and `Stream.empty()` for an empty stream (often as a neutral base case).

* **Stream of Nullable**: `Stream.ofNullable(obj)` (Java 9+) creates a one-element stream if `obj` is non-null, or an empty stream if `obj` is null. This is handy to avoid null checks when streaming possibly null objects.

## Intermediate Operations (Transforming Streams)

Intermediate operations transform a stream into another stream. They are **lazy** (deferred until a terminal operation runs) and typically **stateless** (do not depend on mutable state). Common intermediate operations include:

* **filter(Predicate)** – Select elements that match a condition (predicate). Elements for which the predicate returns false are discarded. (Type and order are preserved, but the count may reduce).
  *Example:* Filter orders with amount > 100:

  ```java
  List<Order> bigOrders = orders.stream()
      .filter(order -> order.getTotal() > 100)
      .toList();  // collect to list (Java 16+)
  ```

  Here, only orders with total over 100 remain in the resulting list.

* **map(Function)** – Transform each element by applying a function, yielding a stream of the new values. The number of elements remains the same, but the type can change.
  *Example:* Transform a list of users to their email addresses:

  ```java
  List<String> emails = users.stream()
      .map(User::getEmail)
      .toList();
  ```

  In this example, a `Stream<User>` is mapped to a `Stream<String>` (email addresses).

* **mapToInt, mapToLong, mapToDouble** – Specialized mapping to primitive streams (`IntStream`, etc.) for performance when dealing with primitives. For instance, `mapToInt(Product::getQuantity)` yields an `IntStream` of quantities.

* **flatMap(Function\<T, Stream<R>>)** – Flatten nested structures by mapping each element to a stream, then flattening those streams into one. Use it when one input element can lead to multiple output elements (one-to-many mapping).
  *Example:* Suppose each `Order` has a list of `LineItem`s. To get a stream of *all* line items from a list of orders:

  ```java
  List<LineItem> allItems = orders.stream()
      .flatMap(order -> order.getLineItems().stream())
      .toList();
  ```

  Here, each order’s `List<LineItem>` is converted to a stream and all such streams are flattened into one continuous stream of line items. This is more convenient than nested loops.

* **mapMulti(BiConsumer\<T, Stream.Builder<R>>)** – *(Java 16+)* A more flexible alternative to flatMap. It allows mapping each element to *zero, one, or many* output elements by using a callback (BiConsumer) to add elements to the downstream. In other words, `mapMulti` combines the functionality of `map` and `flatMap` with finer control.
  *Example:* Split strings by a delimiter into multiple pieces:

  ```java
  List<String> words = Stream.of("apple,banana", "cherry")
      .<String>mapMulti((str, downstream) -> {
          for (String part : str.split(",")) {
              downstream.accept(part);
          }
      })
      .toList();
  // Result: ["apple", "banana", "cherry"]
  ```

  In the above, each input string can contribute multiple outputs (`"apple,banana"` contributes `"apple"` and `"banana"`, while `"cherry"` contributes just `"cherry"`). `mapMulti` was introduced in Java 16 to give this granular control and can be more efficient than flatMap in certain cases by avoiding creation of short-lived streams.

* **distinct()** – Removes duplicate elements (according to `equals()`), yielding a stream of unique elements. *Note:* This is a stateful operation (it must track seen elements) and can impact performance on large streams. Also, ordering is preserved for the remaining elements.

* **sorted()** – Sorts the stream’s elements (in natural order or using a provided `Comparator`). This operation is stateful and will buffer elements. The encounter order of the stream is changed to sorted order.
  *Example:* `employees.stream().sorted(Comparator.comparing(Employee::getName))...`

* **limit(n) and skip(n)** – Truncate or skip elements in a stream. `limit(n)` keeps only the first n elements (if available) and discards the rest, while `skip(n)` skips the first n elements and processes the rest. These are useful for pagination or controlling large streams. They are short-circuiting operations (can cut off processing early once the limit is reached).

* **takeWhile(Predicate)** – *(Java 9+)* Take elements from the stream while a condition is true, and stop at the first false. Once an element fails the predicate, no further elements are considered (even if subsequent ones might match).
  *Example:* `stream.takeWhile(x -> x < 5)` takes elements until an element `>=5` is encountered.

* **dropWhile(Predicate)** – *(Java 9+)* The opposite of takeWhile: drop elements while the condition is true, and once it encounters the first element that fails the predicate, it yields that element and the rest of the stream. Essentially, it skips an initial segment of elements that meet the condition.

* **peek(Consumer)** – Performs an action on each element as it passes through the pipeline, and returns a stream of the *same* elements. It's mainly for debugging or performing incidental actions (like logging) without altering the stream’s contents. *Important:* Because streams are lazy, `peek()` will only execute if a terminal operation happens. Also, avoid using `peek` to mutate the elements – that can be considered a side effect; use it only for observing the flow (e.g., logging intermediate values).

**Example – Chaining Intermediate Operations:**
Here's a pipeline that uses multiple intermediate operations to process data step by step:

```java
// Example: Get the first 5 unique product names (in uppercase) of expensive products (price > 100), sorted alphabetically.
List<String> result = products.stream()
    .filter(p -> p.getPrice() > 100)         // keep only expensive products
    .map(Product::getName)                   // transform to product name
    .map(String::toUpperCase)                // transform name to uppercase
    .distinct()                              // unique names only
    .sorted()                                // sort alphabetically
    .limit(5)                                // take first 5
    .toList();                               // collect to unmodifiable List
```

Each operation is applied in order to transform the data. Because of lazy evaluation, no work is done until the terminal `.toList()` is called – at that point the pipeline runs and produces the result list.

## Terminal Operations (Producing Results)

Terminal operations trigger the execution of the stream pipeline and produce a concrete result or a side effect. After a terminal operation, the stream is consumed and no further operations can be applied. Common terminal operations include:

* **forEach(Consumer)** – Iterates over each element and performs the given action (a Consumer lambda). This is often used to produce side effects (like printing out values or accumulating results in an external structure). For example:

  ```java
  orders.stream()
        .filter(Order::isDelayed)
        .forEach(order -> System.out.println("Delayed: " + order));
  ```

  If the stream is parallel, note that `forEach()` may process elements in arbitrary order. Use `forEachOrdered()` if you need to preserve the encounter order in parallel streams.

* **collect(Collector)** – Reduces the stream into a collection or other container as defined by a **Collector** (from `java.util.stream.Collectors`). This is a very powerful terminal operation, often used to accumulate results into Lists, Sets, Maps, or even to aggregate summary statistics. We cover collectors in detail in the next section.

* **toList()** – A convenient terminal operation (added in Java 16) that collects the stream into an unmodifiable `List` directly, without needing an explicit Collector. In the above examples, we used `.toList()` to get a result list. Keep in mind the returned list is unmodifiable (attempting to add to it will throw an exception). If you need a mutable list, use `collect(Collectors.toList())` (which *typically* returns an `ArrayList`, though not guaranteed to be mutable or any specific type).

* **reduce(identity, BinaryOperator)** – Reduces the stream to a single value by repeatedly applying a binary operation to accumulate elements. There are a few overloaded forms of `reduce()`:

  * `reduce(identity, accumulator)` takes a starting identity value and an accumulator function. For example:

    ```java
    int sum = numbers.stream().reduce(0, Integer::sum);
    ```

    This computes the sum of a stream of integers, with 0 as the starting value and `Integer::sum` combining elements.
  * `reduce(BinaryOperator)` (without identity) returns an `Optional<T>` because if the stream is empty, no result is present. e.g. `Optional<Integer> sumOpt = numbers.stream().reduce(Integer::sum);`.
  * `reduce(identity, accumulator, combiner)` is used in parallel processing with a combiner function for merging partial results (usually the same as the accumulator in associative operations).

  Use `reduce` for aggregations when there isn’t a specific collector available. However, many common reductions (sum, min, max, etc.) have specialized operations or collectors:

  * Summing numbers: you can use `mapToInt(...).sum()` or `mapToInt(...).average()`, etc., or collectors like `Collectors.summingInt`.
  * Finding min/max: Use `stream.min(comparator)` or `stream.max(comparator)` which return an Optional with the min or max element.

* **findFirst() / findAny()** – These return an `Optional<T>` describing some element of the stream. `findFirst()` gives the first element in encounter order (useful for sequential streams or when order matters). `findAny()` may return *any* element from the stream and is useful in parallel streams where picking any element can be more efficient (but in sequential, it behaves like findFirst). For example:

  ```java
  Optional<User> anyAdmin = users.stream()
      .filter(User::isAdmin)
      .findAny();
  ```

  If an admin user exists, you get it; otherwise the Optional is empty.

* **anyMatch / allMatch / noneMatch (Predicate)** – These terminal operations do a test on the stream elements:

  * `anyMatch` returns true if *any* element matches the predicate.
  * `allMatch` returns true if *every* element matches the predicate (or the stream is empty).
  * `noneMatch` returns true if *no* elements match the predicate.

  They are short-circuiting: e.g. `anyMatch` will stop as soon as a match is found.
  *Example:*

  ```java
  if (orders.stream().anyMatch(o -> o.getStatus() == Status.DELAYED)) {
      notifyShippingDept();
  }
  ```

  This checks if there is at least one delayed order.

* **count()** – Returns the count of elements in the stream (as a `long`). It’s a special case of reduction that simply tallies elements. This will traverse the entire stream to count (unless combined with short-circuiting operations like limit).

* **min(Comparator) / max(Comparator)** – Terminal ops that find the minimum or maximum element according to a given Comparator, returning an Optional of the result. Use these to avoid manually doing a reduce for min/max.

**Important:** If you forget to terminate a stream (for example, just calling `.filter()` or `.map()` without a terminal op), nothing will happen because of the lazy evaluation. Always ensure you end with a terminal operation, otherwise your pipeline will not execute at all.

## Collectors and Collecting Results

The `Collectors` class (in `java.util.stream.Collectors`) provides a variety of factory methods for common collectors used with the stream `collect()` terminal operation. Collectors are used to gather the elements of a stream into a result container or to perform mutable reductions (like grouping or partitioning data). Below are common usage patterns relevant in backend scenarios:

* **Collect to List or Set**:

  * `Collectors.toList()` – collects into a `List`. (In Java 21, this typically returns an ArrayList, but do not assume mutability; if you need a guaranteed unmodifiable list, use `Stream.toList()` or `Collectors.toUnmodifiableList()`.)
  * `Collectors.toSet()` – collects into a `Set` (typically a HashSet). This will remove duplicates (since a Set can’t contain duplicates) and **does not guarantee order**. If encounter order matters and you want to collect to a set, consider `Collectors.toCollection(LinkedHashSet::new)` to preserve insertion order.
    *Example:*

  ```java
  Set<String> uniqueCities = customers.stream()
      .map(Customer::getCity)
      .collect(Collectors.toSet());
  ```

  This collects unique city names from customers into a HashSet (order not defined).

* **Collect to Specific Collection**: Use `Collectors.toCollection(Supplier)` to specify the exact collection implementation. For example, to collect into a `LinkedList` or a specific list type:

  ```java
  LinkedList<String> namesList = people.stream()
      .map(Person::getName)
      .collect(Collectors.toCollection(LinkedList::new));
  ```

  Or collecting into a TreeSet (which will also sort the elements):

  ```java
  TreeSet<String> sortedNames = people.stream()
      .map(Person::getName)
      .collect(Collectors.toCollection(TreeSet::new));
  ```

* **Collect to Map**: `Collectors.toMap(keyMapper, valueMapper, [mergeFunction], [supplier])` accumulates elements into a `Map`. You must specify how to get the key and value for each stream element.

  * *Important:* If multiple elements map to the same key, you must provide a merge function to resolve collisions, otherwise `toMap` will throw an IllegalStateException for duplicate keys. For example, to map product names to their price:

    ```java
    Map<String, Double> priceByName = products.stream().collect(
        Collectors.toMap(Product::getName, Product::getPrice)
    );
    ```

    If duplicate names exist, this will throw. To handle duplicates (e.g., keep the latest price):

    ```java
    Map<String, Double> priceByName = products.stream().collect(
        Collectors.toMap(Product::getName, 
                         Product::getPrice, 
                         (price1, price2) -> price2)  // if duplicate, use the latter price
    );
    ```
  * You can also supply a specific `Map` implementation if needed as a fourth parameter. For instance, `toMap(..., HashMap::new)` or `toMap(..., LinkedHashMap::new)` to control mutability or ordering of the map.

* **Grouping**: `Collectors.groupingBy(classifier, downstream)` is extremely useful for bucketizing data by a key. It returns a `Map<K, List<T>>` by default (where K is the grouping key type):

  ```java
  // Group employees by department:
  Map<String, List<Employee>> employeesByDept = employees.stream()
      .collect(Collectors.groupingBy(Employee::getDepartment));
  ```

  This will create a map where each department name maps to the list of employees in that department. The classifier function (`Employee::getDepartment`) defines the grouping key. By default, groupingBy uses `Collectors.toList()` for the values.

  You can provide a **downstream collector** to change the result type or perform aggregation on each group. For example:

  ```java
  // Group employees by department and count them in each department
  Map<String, Long> numEmployeesByDept = employees.stream().collect(
      Collectors.groupingBy(Employee::getDepartment, Collectors.counting()));
  ```

  Here, instead of lists, each department key maps to a count of employees in that department.

  You can also do multi-level grouping by using a downstream groupingBy as well (grouping by multiple fields), e.g. first by department, then by job title:

  ```java
  Map<String, Map<String, List<Employee>>> deptThenTitleMap = employees.stream()
      .collect(Collectors.groupingBy(Employee::getDepartment,
          Collectors.groupingBy(Employee::getJobTitle)));
  ```

  This produces a nested map.

* **Partitioning**: `Collectors.partitioningBy(predicate)` is a special case of grouping where the classifier is a boolean predicate. It splits elements into two groups: those that match the predicate and those that don't. It returns a `Map<Boolean, List<T>>`. For example:

  ```java
  Map<Boolean, List<User>> usersByActive = users.stream()
      .collect(Collectors.partitioningBy(User::isActive));
  ```

  `usersByActive.get(true)` would be the list of active users, and `.get(false)` the list of inactive users. You can also supply a downstream collector to partitioningBy similar to groupingBy.

* **Mapping and Filtering within Collectors**: Java 9 introduced collectors to perform additional transformations as part of a collect operation:

  * `Collectors.mapping(func, downstream)` – applies a mapping function to elements **as part of collecting**. For example, to collect names of employees by department:

    ```java
    Map<String, List<String>> namesByDept = employees.stream().collect(
        Collectors.groupingBy(Employee::getDepartment,
            Collectors.mapping(Employee::getName, Collectors.toList()))
    );
    ```

    Here, within each department group, we map each Employee to their name before collecting into a list.

  * `Collectors.filtering(predicate, downstream)` – filters elements as part of collection. For example, group orders by customer, but only include high-value orders in each list:

    ```java
    Map<String, List<Order>> bigOrdersByCustomer = orders.stream().collect(
        Collectors.groupingBy(Order::getCustomerId,
            Collectors.filtering(order -> order.getTotal() > 1000, Collectors.toList()))
    );
    ```

    This will group orders by customer, but each customer's list contains only orders with total > 1000 (others are filtered out during collecting).

  * `Collectors.flatMapping(func, downstream)` – similar to mapping but flattens the resulting streams. If each element can produce multiple values for the group, flatMapping is useful. Example: suppose each employee has a set of skills, and we want a map of department to *all skills* of employees in that department:

    ```java
    Map<String, Set<String>> skillsByDept = employees.stream().collect(
        Collectors.groupingBy(Employee::getDepartment,
            Collectors.flatMapping(emp -> emp.getSkills().stream(), Collectors.toSet()))
    );
    ```

    Here, each employee contributes a stream of their skills, and flatMapping flattens them so that each department’s set is a union of skills of its employees.

* **Joining Strings**: If you want to concatenate stream elements into one string, use `Collectors.joining()`. There are overloads:

  * `Collectors.joining()` with no delimiter (just concatenates),
  * `Collectors.joining(delimiter)` e.g. `Collectors.joining(", ")` to join with commas,
  * `Collectors.joining(delimiter, prefix, suffix)` if you need to add a prefix or suffix to the final string.
    Example:

  ```java
  String csv = products.stream()
      .map(Product::getName)
      .collect(Collectors.joining(", "));
  // e.g. result: "Laptop, Phone, Tablet"
  ```

* **Summarizing and Averaging**: The Collectors class provides convenient collectors to get statistical summaries:

  * `Collectors.summarizingInt(ToIntFunction)` (and `summarizingLong`, `summarizingDouble`) gives an `IntSummaryStatistics` result that includes count, sum, min, max, and average in one go.
  * `Collectors.averagingInt`, `averagingDouble`, etc., compute the average and return a Double.
    Example:

  ```java
  Double avgAge = people.stream().collect(Collectors.averagingInt(Person::getAge));
  IntSummaryStatistics stats = people.stream().collect(Collectors.summarizingInt(Person::getAge));
  System.out.println("Avg age: " + stats.getAverage() + 
                     ", Min age: " + stats.getMin() + 
                     ", Max age: " + stats.getMax());
  ```

* **collectingAndThen(collector, finisher)** – This allows you to perform a finishing transformation on the result of a collection. For example, to get an immutable list (though `Stream.toList()` already gives an unmodifiable list) you could do:

  ```java
  List<String> namesImmutable = people.stream()
      .map(Person::getName)
      .collect(Collectors.collectingAndThen(Collectors.toList(), 
                                            Collections::unmodifiableList));
  ```

  Or you might collect into a list and then get its size, all in one go:

  ```java
  int count = people.stream()
      .filter(Person::isActive)
      .collect(Collectors.collectingAndThen(Collectors.toList(), List::size));
  ```

  In this case, the finisher function takes the intermediate result (the list of active people) and returns its size.

* **Teeing (combining two collectors)** – *(Java 12+)* The `Collectors.teeing(col1, col2, merger)` collector allows you to run two collectors in parallel on the stream and then merge their results. This is useful for computing two different aggregates in one pass.
  *Example:* Suppose we want to produce a single string that lists all product names and also includes the count of products. We can **tee** a joining collector and a counting collector:

  ```java
  String summary = products.stream().collect(Collectors.teeing(
      Collectors.mapping(Product::getName, Collectors.joining(", ")),  // join names
      Collectors.counting(),                                          // count products
      (names, count) -> names + " (Total: " + count + " products)"
  ));
  ```

  If `products` contains names `"A", "B", "C"`, this would result in: `"A, B, C (Total: 3 products)"`. The teeing collector runs both the joining and counting in one traversal and then combines their results.

* **Custom Collectors**: While the built-in collectors cover most needs, you can always implement a custom `Collector`. A collector is defined by functions: a supplier (to create a mutable result container), an accumulator (to add one element to the container), a combiner (to merge two containers, used in parallel processing), and an optional finisher (to transform the container into the final result type, if needed). You can use `Collector.of(supplier, accumulator, combiner, finisher)` to create one.
  *Example:* A custom collector to join strings with a comma (similar to `Collectors.joining`, but for illustration):

  ```java
  Collector<String, StringJoiner, String> toCommaSeparatedString =
      Collector.of(
          () -> new StringJoiner(", "),               // supplier: new StringJoiner
          (j, str) -> j.add(str),                     // accumulator: add string to joiner
          (j1, j2) -> { j1.merge(j2); return j1; },   // combiner: merge two joiners
          StringJoiner::toString                      // finisher: convert joiner to String
      );

  String result = names.stream().collect(toCommaSeparatedString);
  // result looks like "Name1, Name2, Name3"
  ```

  In real scenarios, you might write custom collectors for more complex aggregations that aren’t covered by the standard library, but often the provided collectors (groupingBy, mapping, etc.) and combinations thereof are sufficient.

## Parallel Streams

Using the Stream API in parallel can leverage multiple CPU cores to speed up data processing, but it must be done carefully. You can create a parallel stream in two ways: call `parallelStream()` on a Collection, or call `.parallel()` on an existing Stream instance. For example:

```java
List<Payment> payments = ...;
double total = payments.parallelStream()    // or payments.stream().parallel()
    .filter(p -> p.getStatus() == SUCCESS)
    .mapToDouble(Payment::getAmount)
    .sum();
```

This will internally split the payments list and process chunks in parallel threads, then combine the results to compute the sum.

**How it works:** Java uses a ForkJoinPool (the *common pool* by default) to execute stream operations in parallel. Each thread processes a portion of the data. Keep in mind:

* **Order and Splitting:** If the source has a defined encounter order (like a List), the stream tries to partition the data in balanced chunks. Some operations, like `limit()` or `findFirst()`, enforce ordering which can limit parallel efficiency. If order is not needed, you can call `.unordered()` on the stream to allow more optimization in parallel.
* **Thread Safety:** Avoid operations that modify shared state or are not thread-safe. For example, don’t append to the same `ArrayList` from a parallel `forEach` (this can lead to race conditions). Instead, collect results via thread-safe collectors (the Stream library handles combining results safely when you use standard collectors).
* **Side effects:** As with sequential streams, try to keep the pipeline functions (like map/filter) free of side effects. Side effects in parallel streams can be even more dangerous due to concurrency.
* **Common Pool**: By default, parallel streams use the common ForkJoinPool which has a number of threads equal to the number of processor cores. Blocking operations (I/O, network calls) in a parallel stream can stall those threads. If you have blocking tasks, consider using a custom thread pool or using alternative approaches (like using `CompletableFuture` or explicit thread management) rather than parallel streams, or increase the pool size via system properties.

**When to use parallel**: Only for CPU-intensive operations on large data sets where you have measured a benefit. For small collections or trivial operations, parallel streams can be slower due to thread scheduling overhead. Always benchmark if possible – parallelism is not a silver bullet and can sometimes even degrade performance if misused.

**Example – Caution with shared mutable state:**

```java
List<Integer> list = Collections.synchronizedList(new ArrayList<>());
IntStream.range(0, 1000).parallel().forEach(list::add);
System.out.println(list.size());  // The size might be 1000, but the list could have duplicates or lost updates!
```

In this example, even using a synchronized list, adding in parallel without proper synchronization can lead to issues. It's generally better to avoid mutating shared collections in a parallel stream. If you need to collect results, use `.collect()` with a thread-safe collector or a collector that handles combining (Collectors will handle it for you).

## Best Practices and Common Pitfalls

When using streams in real-world backend scenarios, keep these tips in mind to write efficient and correct code:

* **Always terminate the stream**: If you chain intermediate operations but never call a terminal operation, the pipeline will do nothing. Ensure you end with a terminal op like `collect()`, `forEach()`, etc., to execute the stream.

* **Don’t reuse a stream after a terminal operation**: Streams are single-use. Once a terminal operation is called, the stream is closed and any further attempt to use it will throw an `IllegalStateException`. If you need to traverse data multiple times, you must get a new stream (e.g., call `.stream()` again on the collection or use a stream supplier as shown below). For example:

  ```java
  Stream<String> s = Stream.of("a","b","c");
  s.forEach(System.out::println);
  // s.forEach(x -> ...); // this would throw IllegalStateException – stream already used
  ```

  **Workaround**: If the source is repeatable (like a collection), simply get a new stream. If not, consider collecting to a list or using a `Supplier<Stream>` to generate a fresh stream each time.

* **Avoid side effects in intermediate operations**: A common mistake is doing actions with side effects (modifying external state) inside `map()` or `filter()` instead of using these for pure transformations. Side effects can lead to unpredictable behavior, especially in parallel streams. For instance, don’t write to an external list from inside a `map()`; `map` should transform and return the new value, not modify something. If you need to perform a side effect like adding to a list, do it in a terminal operation (`forEach`) or use a collector to accumulate results. For example, instead of:

  ```java
  List<String> result = new ArrayList<>();
  stream.map(x -> { result.add(transform(x)); return x; }); // Wrong: using map for side-effect
  ```

  do:

  ```java
  List<String> result = stream
      .map(x -> transform(x))
      .collect(Collectors.toList()); // collect results properly
  ```

  or use `forEach` if no new collection is needed:

  ```java
  List<String> result = new ArrayList<>();
  stream.map(x -> transform(x))
        .forEach(result::add);       // side effect in terminal stage (acceptable)
  ```

  Better yet, favor collecting over explicit side-effects. In summary: keep intermediate operations pure (no side effects).

* **Be mindful of nulls**: If there's any chance your stream might contain null elements, using operations like `filter(x -> x.property...)` will throw a NullPointerException when encountering null. Either filter out nulls first (`filter(Objects::nonNull)`), use `ofNullable` when creating streams from potentially null sources, or otherwise handle null values. The `Collectors.toList()` and others will happily accept nulls as elements, but attempting to process null in a lambda will cause issues.

* **Use the right collector for the job**: If you just need to perform an action on each element and don’t need a new collection, prefer `forEach` over `collect(Collectors.toList())` to avoid unnecessary collection creation. Conversely, if you need a result collection or further processing, use collect. Don’t collect just for the sake of it; choose the simplest terminal operation that achieves your goal.

* **Ordering concerns**: If the encounter order matters (for example, processing a list in order), be cautious with operations that can reorder (like `sorted()`) or with parallel streams (which by default preserve order for ordered sources, but certain operations like `forEach` do not guarantee ordered processing without `forEachOrdered`). Also note that collecting into a Set or other unordered collection will lose the original order. If order is important, stick to ordered collectors (like toList or toCollection with a list/linked set) and avoid calling `.unordered()` on the stream.

* **Parallel stream considerations**:

  * Only use parallel if there’s a proven performance benefit. For small streams or simple operations, stick to sequential (the overhead of parallelization can outweigh benefits).
  * Ensure that the operations in the pipeline are thread-safe and preferably stateless. Avoid accessing or modifying shared mutable state from within a parallel stream.
  * If you need to perform a stateful operation in parallel (like collecting to a shared list), use thread-safe mechanisms or, better, use the provided collectors which handle combining results from multiple threads.
  * Remember that parallel streams use the common ForkJoinPool – if your stream operations are blocking or you need a different threading model, you might need to manage threads manually or consider alternatives (like using a custom pool via `ForkJoinPool.commonPool()` tweaks or using `CompletableFuture`).

* **Closing resources**: If a stream is backed by I/O (e.g., `Files.lines()` returns a stream that holds an open file), it implements `AutoCloseable`. Such streams should be closed after use, typically by wrapping in a try-with-resources:

  ```java
  try (Stream<String> lines = Files.lines(path)) {
      long count = lines.filter(line -> line.contains("ERROR")).count();
      // use count...
  }
  ```

  This ensures the underlying file resource is freed. Regular in-memory streams (from collections, etc.) don’t need closing.

By following these best practices and being aware of pitfalls, you can effectively use the Java 21 Stream API to write clean, efficient, and correct backend code. Streams enable concise and powerful data processing, from simple filters to complex groupings, but always remember the functional ethos: describe *what* you want to achieve (filter, transform, collect) and let the Stream API handle the iteration under the hood. Happy streaming!

**Sources:** Java official documentation and community tutorials on Streams. The guidelines and examples above incorporate improvements up to Java 21 (including features from Java 9, 16, and beyond) to provide a modern and practical reference for developers.
